{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index\n",
    "%pip install llama-index-llms\n",
    "%pip install llama-index-readers\n",
    "%pip install llama-index-embeddings\n",
    "%pip install dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "from llama_index.core import ( Settings, VectorStoreIndex, SimpleDirectoryReader)\n",
    "from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler\n",
    "from llama_index.core.extractors import ( SummaryExtractor, QuestionsAnsweredExtractor, TitleExtractor, KeywordExtractor)\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.question_gen import LLMQuestionGenerator\n",
    "from llama_index.core.question_gen.prompts import (DEFAULT_SUB_QUESTION_PROMPT_TMPL)\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "\n",
    "Settings.callback_manager = callback_manager\n",
    "\n",
    "load_dotenv('../Credentials/.env')\n",
    "\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "credential = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_api_version = \"2024-04-01-preview\"\n",
    "azure_openai_embedding_deployment = \"text-embedding-ada-002\"\n",
    "embedding_model_name = \"text-embedding-ada-002\"\n",
    "llm_model_name = \"gpt-35-turbo-16k\"\n",
    "api_type = \"azure\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(\"../Data/\", recursive=True, filename_as_id=True, required_exts=[\".pdf\", \".docx\", \".xlsx\", \".pptx\"])\n",
    "\n",
    "documents = []\n",
    "for docs in reader.iter_data():\n",
    "    documents.extend(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(\n",
    "            model = llm_model_name,\n",
    "            deployment_name = llm_model_name,\n",
    "            api_key = credential,\n",
    "            azure_endpoint = endpoint,\n",
    "            api_version = azure_openai_api_version,\n",
    "            api_type = api_type\n",
    "        )\n",
    "\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "            model = embedding_model_name,\n",
    "            deployment_name = embedding_model_name,\n",
    "            api_key = credential,\n",
    "            azure_endpoint = endpoint,\n",
    "            api_version = azure_openai_api_version,\n",
    "            api_type = api_type,\n",
    "            embed_batch_size=50\n",
    "        )\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "    separator=\" \", chunk_size=512, chunk_overlap=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extractors = [\n",
    "    TitleExtractor(nodes=2, llm=llm),\n",
    "    # QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
    "    # EntityExtractor(prediction_threshold=0.5),\n",
    "    # SummaryExtractor(summaries=[\"prev\", \"self\"], llm=llm),\n",
    "    # KeywordExtractor(keywords=3, llm=llm),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = [text_splitter] + extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = IngestionPipeline(transformations=transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 429/429 [02:18<00:00,  3.10it/s]\n",
      "  4%|▍         | 17/429 [00:02<00:46,  8.80it/s]Exception in thread Thread-22:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py\", line 576, in _handle_results\n",
      "    task = get()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 256, in recv\n",
      "    return _ForkingPickler.loads(buf.getbuffer())\n",
      "TypeError: __init__() missing 2 required keyword-only arguments: 'response' and 'body'\n",
      "\n",
      " 82%|████████▏ | 410/503 [02:22<00:40,  2.29it/s]\n",
      " 77%|███████▋  | 402/523 [02:21<00:40,  3.02it/s]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.29it/s]\u001b[A\n",
      "\n",
      " 82%|████████▏ | 411/503 [02:22<00:45,  2.01it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  6.12it/s]\u001b[A\n",
      "100%|██████████| 445/445 [02:25<00:00,  3.05it/s]\n",
      "100%|██████████| 503/503 [02:49<00:00,  2.96it/s]\n",
      "100%|██████████| 523/523 [03:02<00:00,  2.87it/s]\n",
      "100%|██████████| 445/445 [00:38<00:00, 11.64it/s]\n",
      "100%|██████████| 503/503 [00:42<00:00, 11.93it/s]\n",
      "100%|██████████| 523/523 [00:44<00:00, 11.63it/s]\n"
     ]
    }
   ],
   "source": [
    "nodes = pipeline.run(documents=documents, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LLM sees:\\n\",(nodes)[9].get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex(nodes)\n",
    "\n",
    "engine = index.as_query_engine(similarity_top_k=10, llm=llm)\n",
    "\n",
    "index.storage_context.persist(persist_dir=\"./data2/index\")\n",
    "\n",
    "question_gen = LLMQuestionGenerator.from_defaults(\n",
    "    llm=llm,\n",
    "    prompt_template_str=\"\"\"\n",
    "        Follow the example, but instead of giving a question, always prefix the question \n",
    "        with: 'By first identifying and quoting the most relevant sources, '. \n",
    "        \"\"\"\n",
    "    + DEFAULT_SUB_QUESTION_PROMPT_TMPL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to Persistent Storage\n",
    "\n",
    "In case you want to load your index later, saving you from having to re-parse your documents every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir=\"../Data/course113113_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Query Engine, Ask a Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"course_documents\",\n",
    "            description=\"course files from IHP1\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    question_gen=question_gen,\n",
    "    use_async=False\n",
    ")\n",
    "\n",
    "\n",
    "# query_engine = index.as_query_engine(similarity_top_k=5)\n",
    "\n",
    "query = (\n",
    "    'Give me a 3 sentence summary of Hemodynamics'\n",
    ")\n",
    "\n",
    "query_response = query_engine.query(\n",
    "    query\n",
    ")\n",
    "\n",
    "print(query_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
